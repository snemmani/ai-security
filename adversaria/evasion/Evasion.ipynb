{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/jdHACptfuNKj1BcMu1n4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snemmani/ai-security/blob/main/adversaria/evasion/Evasion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fast Gradient Sign Method\n",
        "\n",
        "## The Concept: Training vs. Attacking\n",
        "In a normal training loop, we freeze the input data and update the model weights to minimize the loss (error).\n",
        "In an evasion attack (like FGSM), we flip the script:We freeze the model weights.We update the input data to maximize the loss.\n",
        "\n",
        "We want to find a tiny perturbation (let's call it $\\delta$) that we can add to an image $x$, such that the human eye sees the same image, but the model sees something completely different.The Math: Fast Gradient Sign Method (FGSM)The most common \"first attack\" to learn is the Fast Gradient Sign Method (FGSM).\n",
        "\n",
        "It uses the gradients of the neural network to create an adversarial example.The formula looks like this:\n",
        "\n",
        "$$x_{adv} = x + \\epsilon \\cdot \\text{sign}(\\nabla_x J(\\theta, x, y))$$\n",
        "\n",
        "TensorFlow with MNIST is the classic \"Hello World\" of adversarial ML. It's lightweight, so we can iterate quickly.\n",
        "\n",
        "## The Victim Model ğŸ¯"
      ],
      "metadata": {
        "id": "Rw7yfxNEQDX5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdpYJvzqDBX7",
        "outputId": "d00e48d7-9829-4414-e6b1-118a1928a6bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Shape of training images: (60000, 28, 28)\n",
            "Shape of training labels: (60000,)\n",
            "Shape of test images: (10000, 28, 28)\n",
            "Shape of test labels: (10000,)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "print(f\"Shape of training images: {x_train.shape}\")\n",
        "print(f\"Shape of training labels: {y_train.shape}\")\n",
        "print(f\"Shape of test images: {x_test.shape}\")\n",
        "print(f\"Shape of test labels: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7868559"
      },
      "source": [
        "### Task\n",
        "Build, compile, train, and evaluate a Deep Neural Network (DNN) model for image classification using the MNIST dataset, then present the accuracy, precision, and recall metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3d140e9"
      },
      "source": [
        "### Preprocess Data\n",
        "\n",
        "#### Subtask:\n",
        "Normalize and reshape the `x_train` and `x_test` images from 28x28 to 784-dimensional vectors, and convert labels `y_train` and `y_test` to one-hot encoded format.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6accf8b1",
        "outputId": "57aba3e9-bee7-4882-b329-4674c186752e"
      },
      "source": [
        "x_train_normalized = x_train.astype('float32') / 255.0\n",
        "x_test_normalized = x_test.astype('float32') / 255.0\n",
        "\n",
        "x_train_reshaped = x_train_normalized.reshape((60000, 784))\n",
        "x_test_reshaped = x_test_normalized.reshape((10000, 784))\n",
        "\n",
        "y_train_one_hot = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test_one_hot = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
        "\n",
        "print(f\"Shape of normalized and reshaped training images: {x_train_reshaped.shape}\")\n",
        "print(f\"Shape of normalized and reshaped test images: {x_test_reshaped.shape}\")\n",
        "print(f\"Shape of one-hot encoded training labels: {y_train_one_hot.shape}\")\n",
        "print(f\"Shape of one-hot encoded test labels: {y_test_one_hot.shape}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of normalized and reshaped training images: (60000, 784)\n",
            "Shape of normalized and reshaped test images: (10000, 784)\n",
            "Shape of one-hot encoded training labels: (60000, 10)\n",
            "Shape of one-hot encoded test labels: (10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07713459"
      },
      "source": [
        "### Build DNN Model\n",
        "\n",
        "#### Subtask:\n",
        "Construct a Deep Neural Network (DNN) using TensorFlow/Keras with appropriate Dense layers for image classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85f5a863"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to construct a Deep Neural Network (DNN) model using TensorFlow/Keras following the provided instructions for layer configuration and activation functions, and then display its summary. All these operations can be performed in a single code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "974e2615",
        "outputId": "9fcef853-92a2-4f3c-cf0a-0c971bf18ad2"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "\n",
        "# Initialize a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add an Input layer to explicitly define the input shape\n",
        "model.add(Input(shape=(784,)))\n",
        "\n",
        "# Add the first Dense layer\n",
        "model.add(Dense(256, activation='relu'))\n",
        "\n",
        "# Add a second Dense hidden layer\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# Add the output Dense layer\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ dense_12 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â”‚       \u001b[38;5;34m200,960\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_13 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚        \u001b[38;5;34m32,896\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_14 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             â”‚         \u001b[38;5;34m1,290\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">200,960</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m235,146\u001b[0m (918.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">235,146</span> (918.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m235,146\u001b[0m (918.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">235,146</span> (918.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "103a7b37"
      },
      "source": [
        "### Compile Model\n",
        "\n",
        "#### Subtask:\n",
        "Compile the DNN model, specifying an optimizer (e.g., Adam), a loss function (e.g., 'categorical_crossentropy'), and metrics including 'accuracy', 'Precision', and 'Recall'.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9fcf0c7"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires compiling the DNN model with a specified optimizer, loss function, and metrics. This can be achieved using the `compile` method on the `model` object in a single code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f05a049",
        "outputId": "2a9e35d9-aed6-4585-fe25-6c1d525b58c9"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        ")\n",
        "\n",
        "print(\"Model compiled successfully with Adam optimizer, categorical_crossentropy loss, and accuracy, Precision, Recall metrics.\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model compiled successfully with Adam optimizer, categorical_crossentropy loss, and accuracy, Precision, Recall metrics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d570132"
      },
      "source": [
        "### Train Model\n",
        "\n",
        "#### Subtask:\n",
        "Train the compiled DNN model using the preprocessed `x_train` and `y_train` data for a specified number of epochs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "286f1607"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to train the compiled DNN model. This involves using the `.fit()` method with the preprocessed training data, specifying epochs, batch size, and a validation split, and storing the training history.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e375e37",
        "outputId": "06e6d6f7-eb22-4e16-89e5-914f95432bfa"
      },
      "source": [
        "history = model.fit(\n",
        "    x_train_reshaped,\n",
        "    y_train_one_hot,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "print(\"Model training complete.\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.8878 - loss: 0.3922 - precision_1: 0.9370 - recall_1: 0.8294 - val_accuracy: 0.9658 - val_loss: 0.1105 - val_precision_1: 0.9707 - val_recall_1: 0.9612\n",
            "Epoch 2/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.9706 - loss: 0.0947 - precision_1: 0.9750 - recall_1: 0.9661 - val_accuracy: 0.9688 - val_loss: 0.1020 - val_precision_1: 0.9722 - val_recall_1: 0.9657\n",
            "Epoch 3/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.9821 - loss: 0.0593 - precision_1: 0.9846 - recall_1: 0.9798 - val_accuracy: 0.9743 - val_loss: 0.0911 - val_precision_1: 0.9772 - val_recall_1: 0.9722\n",
            "Epoch 4/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.9856 - loss: 0.0458 - precision_1: 0.9869 - recall_1: 0.9842 - val_accuracy: 0.9714 - val_loss: 0.1049 - val_precision_1: 0.9739 - val_recall_1: 0.9700\n",
            "Epoch 5/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9891 - loss: 0.0329 - precision_1: 0.9898 - recall_1: 0.9881 - val_accuracy: 0.9743 - val_loss: 0.0976 - val_precision_1: 0.9758 - val_recall_1: 0.9726\n",
            "Epoch 6/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.9908 - loss: 0.0262 - precision_1: 0.9916 - recall_1: 0.9901 - val_accuracy: 0.9769 - val_loss: 0.1047 - val_precision_1: 0.9784 - val_recall_1: 0.9756\n",
            "Epoch 7/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.9929 - loss: 0.0224 - precision_1: 0.9934 - recall_1: 0.9925 - val_accuracy: 0.9752 - val_loss: 0.1168 - val_precision_1: 0.9763 - val_recall_1: 0.9746\n",
            "Epoch 8/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.9937 - loss: 0.0185 - precision_1: 0.9942 - recall_1: 0.9933 - val_accuracy: 0.9785 - val_loss: 0.0975 - val_precision_1: 0.9799 - val_recall_1: 0.9780\n",
            "Epoch 9/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.9957 - loss: 0.0140 - precision_1: 0.9960 - recall_1: 0.9955 - val_accuracy: 0.9783 - val_loss: 0.0997 - val_precision_1: 0.9795 - val_recall_1: 0.9774\n",
            "Epoch 10/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.9959 - loss: 0.0129 - precision_1: 0.9962 - recall_1: 0.9957 - val_accuracy: 0.9751 - val_loss: 0.1211 - val_precision_1: 0.9756 - val_recall_1: 0.9747\n",
            "Model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Gradient tape to watch the gradients to see where there is loss in the model weights while predicting the image. Based on the tiny perturbations in the image values, it identifies the loss and its direction.\n"
      ],
      "metadata": {
        "id": "db6OKI_2RZoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_adversarial_pattern(input_image, input_label, model):\n",
        "  input_image_tensor = tf.convert_to_tensor(input_image)\n",
        "  # Add a batch dimension to the input image (from (784,) to (1, 784))\n",
        "  input_image_tensor = tf.expand_dims(input_image_tensor, axis=0)\n",
        "\n",
        "  # Add a batch dimension to the input label (from (10,) to (1, 10)) for loss calculation\n",
        "  input_label_tensor = tf.convert_to_tensor(input_label)\n",
        "  input_label_tensor = tf.expand_dims(input_label_tensor, axis=0)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    tape.watch(input_image_tensor)\n",
        "    # Use direct model call instead of predict to ensure gradients are recorded\n",
        "    prediction = model(input_image_tensor)\n",
        "    # Use the batched label tensor for loss calculation\n",
        "    loss = tf.keras.losses.categorical_crossentropy(input_label_tensor, prediction)\n",
        "    gradient = tape.gradient(loss, input_image_tensor)\n",
        "  signed_grad = tf.sign(gradient)\n",
        "  # Remove the batch dimension before returning the gradient if only a single gradient is expected\n",
        "  return tf.squeeze(signed_grad, axis=0)"
      ],
      "metadata": {
        "id": "Sutmr_-DFX0-"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_adversarial_pattern(x_train_reshaped[0], y_train_one_hot[0], model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zh35fw4dFwWQ",
        "outputId": "be0deb71-f6ff-4917-98ad-5b90354897d6"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(784,), dtype=float32, numpy=\n",
              "array([-1.,  1.,  1., -1., -1.,  1., -1., -1.,  1., -1., -1., -1., -1.,\n",
              "        1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1., -1.,  1., -1., -1.,\n",
              "       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,\n",
              "        1.,  1.,  1.,  1.,  1., -1., -1., -1., -1., -1.,  1.,  1., -1.,\n",
              "        1., -1., -1., -1., -1., -1.,  1.,  1.,  1., -1.,  1., -1., -1.,\n",
              "        1., -1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1., -1., -1., -1.,\n",
              "       -1., -1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,\n",
              "       -1., -1.,  1., -1.,  1., -1., -1.,  1., -1., -1., -1., -1., -1.,\n",
              "        1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,\n",
              "        1., -1.,  1., -1., -1.,  1., -1., -1., -1., -1.,  1., -1., -1.,\n",
              "        1.,  1.,  1.,  1., -1.,  1., -1., -1., -1., -1.,  1., -1., -1.,\n",
              "        1.,  1.,  1., -1., -1., -1., -1., -1.,  1.,  1.,  1., -1., -1.,\n",
              "       -1., -1., -1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1., -1.,\n",
              "       -1., -1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1., -1.,  1.,\n",
              "       -1., -1., -1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1., -1.,\n",
              "       -1., -1.,  1., -1.,  1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1.,\n",
              "        1., -1.,  1., -1.,  1., -1.,  1., -1.,  1., -1., -1., -1.,  1.,\n",
              "        1., -1., -1.,  1., -1., -1., -1.,  1.,  1., -1., -1.,  1., -1.,\n",
              "       -1.,  1.,  1., -1., -1., -1., -1., -1., -1.,  1.,  1.,  1., -1.,\n",
              "        1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1., -1.,\n",
              "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1., -1.,\n",
              "        1.,  1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1., -1.,\n",
              "       -1.,  1., -1.,  1., -1.,  1., -1., -1., -1., -1.,  1., -1.,  1.,\n",
              "       -1., -1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1.,\n",
              "        1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1., -1.,\n",
              "       -1.,  1., -1., -1.,  1., -1.,  1., -1.,  1., -1.,  1., -1.,  1.,\n",
              "        1., -1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1.,  1., -1.,\n",
              "       -1., -1.,  1., -1., -1.,  1., -1., -1., -1.,  1.,  1.,  1.,  1.,\n",
              "        1., -1.,  1., -1., -1., -1., -1., -1., -1.,  1., -1.,  1., -1.,\n",
              "        1., -1.,  1., -1., -1., -1., -1.,  1., -1., -1., -1., -1.,  1.,\n",
              "       -1.,  1., -1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
              "        1., -1.,  1.,  1., -1.,  1., -1., -1.,  1.,  1., -1., -1., -1.,\n",
              "       -1., -1.,  1.,  1., -1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1.,\n",
              "       -1.,  1., -1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,\n",
              "       -1.,  1., -1.,  1.,  1., -1., -1., -1., -1., -1.,  1., -1.,  1.,\n",
              "        1.,  1.,  1., -1.,  1., -1., -1., -1.,  1.,  1.,  1., -1., -1.,\n",
              "       -1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1., -1.,\n",
              "        1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1., -1., -1.,  1.,\n",
              "       -1., -1., -1., -1.,  1., -1., -1., -1., -1., -1.,  1., -1., -1.,\n",
              "        1.,  1., -1.,  1., -1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,\n",
              "       -1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1.,\n",
              "       -1.,  1., -1., -1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1., -1.,\n",
              "        1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1.,  1.,\n",
              "       -1., -1., -1.,  1.,  1., -1., -1., -1., -1.,  1., -1., -1., -1.,\n",
              "        1.,  1.,  1.,  1.,  1., -1., -1., -1., -1., -1., -1.,  1.,  1.,\n",
              "       -1., -1., -1., -1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1.,\n",
              "        1., -1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1., -1., -1.,\n",
              "        1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1., -1., -1., -1.,\n",
              "       -1.,  1.,  1., -1., -1., -1.,  1., -1., -1., -1.,  1., -1.,  1.,\n",
              "        1., -1.,  1., -1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,  1.,\n",
              "        1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1.,\n",
              "        1.,  1.,  1.,  1., -1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,\n",
              "       -1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1.,  1.,\n",
              "        1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1., -1., -1.,  1.,  1.,\n",
              "        1., -1., -1., -1.,  1., -1.,  1., -1.,  1., -1.,  1., -1.,  1.,\n",
              "        1.,  1., -1.,  1., -1., -1.,  1.,  1., -1.,  1., -1., -1., -1.,\n",
              "       -1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1.,\n",
              "        1., -1.,  1., -1.,  1., -1., -1.,  1., -1.,  1., -1., -1.,  1.,\n",
              "       -1., -1., -1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1.,\n",
              "        1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,\n",
              "        1.,  1.,  1., -1.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_adversarial_image(input_image, input_label, model, epsilon=0.01):\n",
        "  gradients = create_adversarial_pattern(input_image, input_label, model)\n",
        "  adversarial_image = input_image + epsilon * gradients\n",
        "  return adversarial_image"
      ],
      "metadata": {
        "id": "OFmvxl-sFy84"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_adversarial_image(x_train_reshaped[0], y_train_one_hot[0], model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXJOOg5KG8Pt",
        "outputId": "bb7d539f-c4a5-4746-df19-49fafb12380e"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(784,), dtype=float32, numpy=\n",
              "array([-0.01      ,  0.01      ,  0.01      , -0.01      , -0.01      ,\n",
              "        0.01      , -0.01      , -0.01      ,  0.01      , -0.01      ,\n",
              "       -0.01      , -0.01      , -0.01      ,  0.01      ,  0.01      ,\n",
              "        0.01      , -0.01      , -0.01      ,  0.01      ,  0.01      ,\n",
              "        0.01      , -0.01      , -0.01      ,  0.01      , -0.01      ,\n",
              "       -0.01      , -0.01      , -0.01      ,  0.01      ,  0.01      ,\n",
              "        0.01      , -0.01      ,  0.01      ,  0.01      ,  0.01      ,\n",
              "        0.01      ,  0.01      ,  0.01      , -0.01      ,  0.01      ,\n",
              "        0.01      ,  0.01      ,  0.01      ,  0.01      , -0.01      ,\n",
              "       -0.01      , -0.01      , -0.01      , -0.01      ,  0.01      ,\n",
              "        0.01      , -0.01      ,  0.01      , -0.01      , -0.01      ,\n",
              "       -0.01      , -0.01      , -0.01      ,  0.01      ,  0.01      ,\n",
              "        0.01      , -0.01      ,  0.01      , -0.01      , -0.01      ,\n",
              "        0.01      , -0.01      ,  0.01      ,  0.01      , -0.01      ,\n",
              "        0.01      ,  0.01      , -0.01      , -0.01      ,  0.01      ,\n",
              "       -0.01      , -0.01      , -0.01      , -0.01      , -0.01      ,\n",
              "       -0.01      , -0.01      , -0.01      ,  0.01      ,  0.01      ,\n",
              "        0.01      ,  0.01      ,  0.01      ,  0.01      ,  0.01      ,\n",
              "       -0.01      , -0.01      , -0.01      ,  0.01      , -0.01      ,\n",
              "        0.01      , -0.01      , -0.01      ,  0.01      , -0.01      ,\n",
              "       -0.01      , -0.01      , -0.01      , -0.01      ,  0.01      ,\n",
              "        0.01      , -0.01      ,  0.01      , -0.01      ,  0.01      ,\n",
              "        0.01      ,  0.01      , -0.01      ,  0.01      ,  0.01      ,\n",
              "        0.01      ,  0.01      ,  0.01      , -0.01      ,  0.01      ,\n",
              "       -0.01      , -0.01      ,  0.01      , -0.01      , -0.01      ,\n",
              "       -0.01      , -0.01      ,  0.01      , -0.01      , -0.01      ,\n",
              "        0.01      ,  0.01      ,  0.01      ,  0.01      , -0.01      ,\n",
              "        0.01      , -0.01      , -0.01      , -0.01      , -0.01      ,\n",
              "        0.01      , -0.01      , -0.01      ,  0.01      ,  0.01      ,\n",
              "        0.01      , -0.01      , -0.01      , -0.01      , -0.01      ,\n",
              "       -0.01      ,  0.01      ,  0.02176471,  0.08058824,  0.06058824,\n",
              "        0.06058824,  0.48411766,  0.5233334 ,  0.67627454,  0.11196078,\n",
              "        0.6409804 ,  0.99      ,  0.97862744,  0.50803924, -0.01      ,\n",
              "        0.01      ,  0.01      ,  0.01      , -0.01      , -0.01      ,\n",
              "       -0.01      , -0.01      ,  0.01      ,  0.01      , -0.01      ,\n",
              "       -0.01      ,  0.10764706,  0.13117647,  0.35862747,  0.5939216 ,\n",
              "        0.6566667 ,  1.0021569 ,  0.9821569 ,  0.9821569 ,  0.9821569 ,\n",
              "        0.9821569 ,  0.87235296,  0.6845098 ,  1.0021569 ,  0.9590196 ,\n",
              "        0.7747059 ,  0.2409804 ,  0.01      , -0.01      , -0.01      ,\n",
              "       -0.01      , -0.01      ,  0.01      , -0.01      ,  0.01      ,\n",
              "        0.01      ,  0.01      , -0.01      ,  0.18215686,  0.9433333 ,\n",
              "        0.9821569 ,  1.0021569 ,  1.0021569 ,  1.0021569 ,  0.9821569 ,\n",
              "        1.0021569 ,  0.9821569 ,  1.0021569 ,  0.97431374,  0.37470588,\n",
              "        0.31156865,  0.33156863,  0.20960784,  0.14294118, -0.01      ,\n",
              "        0.01      ,  0.01      , -0.01      , -0.01      ,  0.01      ,\n",
              "       -0.01      , -0.01      , -0.01      ,  0.01      ,  0.01      ,\n",
              "       -0.01      ,  0.06058824,  0.8688235 ,  0.9821569 ,  0.9821569 ,\n",
              "        1.0021569 ,  1.0021569 ,  0.9821569 ,  0.7664706 ,  0.7037255 ,\n",
              "        0.95862746,  0.93509805, -0.01      ,  0.01      ,  0.01      ,\n",
              "        0.01      , -0.01      ,  0.01      ,  0.01      , -0.01      ,\n",
              "        0.01      ,  0.01      ,  0.01      , -0.01      ,  0.01      ,\n",
              "        0.01      , -0.01      , -0.01      ,  0.01      , -0.01      ,\n",
              "        0.3037255 ,  0.60176474,  0.40960786,  0.9821569 ,  0.9821569 ,\n",
              "        0.7939216 ,  0.03313725, -0.01      ,  0.15862745,  0.5939216 ,\n",
              "        0.01      ,  0.01      , -0.01      ,  0.01      ,  0.01      ,\n",
              "       -0.01      ,  0.01      , -0.01      ,  0.01      , -0.01      ,\n",
              "       -0.01      , -0.01      , -0.01      , -0.01      ,  0.01      ,\n",
              "       -0.01      , -0.01      ,  0.01      , -0.01      ,  0.06490196,\n",
              "       -0.00607843,  0.6139216 ,  0.9821569 ,  0.3429412 , -0.01      ,\n",
              "       -0.01      ,  0.01      , -0.01      ,  0.01      , -0.01      ,\n",
              "       -0.01      , -0.01      ,  0.01      , -0.01      ,  0.01      ,\n",
              "        0.01      ,  0.01      , -0.01      ,  0.01      , -0.01      ,\n",
              "       -0.01      ,  0.01      ,  0.01      ,  0.01      ,  0.01      ,\n",
              "       -0.01      , -0.01      , -0.01      ,  0.01      ,  0.55509806,\n",
              "        0.9821569 ,  0.75509804, -0.00215686,  0.01      , -0.01      ,\n",
              "       -0.01      ,  0.01      , -0.01      , -0.01      ,  0.01      ,\n",
              "       -0.01      ,  0.01      , -0.01      ,  0.01      , -0.01      ,\n",
              "        0.01      , -0.01      ,  0.01      ,  0.01      , -0.01      ,\n",
              "        0.01      , -0.01      , -0.01      ,  0.01      , -0.01      ,\n",
              "        0.01      , -0.01      ,  0.03313725,  0.73509806,  1.0021569 ,\n",
              "        0.26450983, -0.01      , -0.01      ,  0.01      , -0.01      ,\n",
              "       -0.01      ,  0.01      , -0.01      , -0.01      , -0.01      ,\n",
              "        0.01      ,  0.01      ,  0.01      ,  0.01      ,  0.01      ,\n",
              "       -0.01      ,  0.01      , -0.01      , -0.01      , -0.01      ,\n",
              "       -0.01      , -0.01      , -0.01      ,  0.01      , -0.01      ,\n",
              "        0.01      ,  0.1272549 ,  0.95509803,  0.87235296,  0.637451  ,\n",
              "        0.41352943, -0.00607843, -0.01      , -0.01      ,  0.01      ,\n",
              "       -0.01      , -0.01      , -0.01      , -0.01      ,  0.01      ,\n",
              "       -0.01      ,  0.01      , -0.01      , -0.01      , -0.01      ,\n",
              "        0.01      , -0.01      ,  0.01      ,  0.01      ,  0.01      ,\n",
              "        0.01      , -0.01      ,  0.01      ,  0.01      , -0.01      ,\n",
              "        0.32764706,  0.95117646,  0.9821569 ,  1.0021569 ,  0.45666668,\n",
              "        0.08803922,  0.01      ,  0.01      , -0.01      , -0.01      ,\n",
              "       -0.01      , -0.01      , -0.01      ,  0.01      ,  0.01      ,\n",
              "       -0.01      , -0.01      , -0.01      ,  0.01      , -0.01      ,\n",
              "        0.01      ,  0.01      ,  0.01      ,  0.01      , -0.01      ,\n",
              "        0.01      , -0.01      , -0.01      ,  0.01      ,  0.1864706 ,\n",
              "        0.7394118 ,  1.0021569 ,  1.0021569 ,  0.5782353 ,  0.09588236,\n",
              "       -0.01      , -0.01      , -0.01      ,  0.01      , -0.01      ,\n",
              "        0.01      ,  0.01      , -0.01      , -0.01      , -0.01      ,\n",
              "       -0.01      , -0.01      ,  0.01      , -0.01      ,  0.01      ,\n",
              "        0.01      ,  0.01      ,  0.01      , -0.01      ,  0.01      ,\n",
              "       -0.01      , -0.01      , -0.01      ,  0.0727451 ,  0.37470588,\n",
              "        0.9982353 ,  0.9821569 ,  0.72333336, -0.01      ,  0.01      ,\n",
              "        0.01      , -0.01      ,  0.01      ,  0.01      , -0.01      ,\n",
              "        0.01      ,  0.01      , -0.01      ,  0.01      , -0.01      ,\n",
              "       -0.01      ,  0.01      ,  0.01      ,  0.01      , -0.01      ,\n",
              "        0.01      , -0.01      ,  0.01      , -0.01      ,  0.01      ,\n",
              "        0.01      , -0.01      , -0.01      ,  0.9864706 ,  0.9821569 ,\n",
              "        0.9664706 ,  0.2409804 , -0.01      ,  0.01      , -0.01      ,\n",
              "       -0.01      , -0.01      , -0.01      , -0.01      ,  0.01      ,\n",
              "       -0.01      , -0.01      ,  0.01      ,  0.01      , -0.01      ,\n",
              "        0.01      , -0.01      ,  0.01      , -0.01      ,  0.01      ,\n",
              "       -0.01      , -0.01      ,  0.01      ,  0.17039216,  0.51980394,\n",
              "        0.7076471 ,  1.0021569 ,  0.9821569 ,  0.8017647 , -0.00215686,\n",
              "        0.01      ,  0.01      , -0.01      ,  0.01      , -0.01      ,\n",
              "        0.01      , -0.01      ,  0.01      , -0.01      ,  0.01      ,\n",
              "       -0.01      , -0.01      ,  0.01      ,  0.01      , -0.01      ,\n",
              "       -0.01      ,  0.01      ,  0.01      , -0.01      ,  0.16294119,\n",
              "        0.5703922 ,  0.9080392 ,  0.9821569 ,  1.0021569 ,  1.0021569 ,\n",
              "        0.99039215,  0.7237255 ,  0.01      ,  0.01      ,  0.01      ,\n",
              "        0.01      , -0.01      , -0.01      ,  0.01      , -0.01      ,\n",
              "       -0.01      , -0.01      ,  0.01      ,  0.01      , -0.01      ,\n",
              "       -0.01      , -0.01      , -0.01      ,  0.01      , -0.01      ,\n",
              "        0.08411765,  0.43705884,  0.87666667,  1.0021569 ,  1.0021569 ,\n",
              "        1.0021569 ,  1.0021569 ,  0.7782353 ,  0.29588237, -0.01      ,\n",
              "       -0.01      , -0.01      , -0.01      ,  0.01      ,  0.01      ,\n",
              "       -0.01      , -0.01      , -0.01      , -0.01      ,  0.01      ,\n",
              "        0.01      ,  0.01      , -0.01      , -0.01      ,  0.01      ,\n",
              "        0.01      ,  0.10019608,  0.26882353,  0.8452941 ,  0.9821569 ,\n",
              "        0.9821569 ,  1.0021569 ,  0.9821569 ,  0.7864706 ,  0.32764706,\n",
              "       -0.00215686,  0.01      ,  0.01      ,  0.01      , -0.01      ,\n",
              "       -0.01      ,  0.01      , -0.01      , -0.01      , -0.01      ,\n",
              "       -0.01      , -0.01      , -0.01      , -0.01      ,  0.01      ,\n",
              "        0.01      , -0.01      ,  0.06058824,  0.66058826,  0.84882355,\n",
              "        1.0021569 ,  1.0021569 ,  0.9821569 ,  0.9821569 ,  0.7547059 ,\n",
              "        0.3237255 ,  0.02529412, -0.01      , -0.01      ,  0.01      ,\n",
              "       -0.01      ,  0.01      ,  0.01      , -0.01      ,  0.01      ,\n",
              "       -0.01      ,  0.01      , -0.01      ,  0.01      , -0.01      ,\n",
              "       -0.01      ,  0.01      , -0.01      ,  0.22568628,  0.6845098 ,\n",
              "        0.8962745 ,  1.0021569 ,  1.0021569 ,  0.9821569 ,  0.9821569 ,\n",
              "        0.96686274,  0.53156865,  0.05313726,  0.01      ,  0.01      ,\n",
              "        0.01      , -0.01      , -0.01      ,  0.01      ,  0.01      ,\n",
              "        0.01      ,  0.01      , -0.01      , -0.01      ,  0.01      ,\n",
              "       -0.01      , -0.01      ,  0.01      , -0.01      , -0.01      ,\n",
              "        0.01      ,  0.5233334 ,  1.0021569 ,  1.0021569 ,  0.9821569 ,\n",
              "        0.84137255,  0.5194118 ,  0.5276471 ,  0.0527451 ,  0.01      ,\n",
              "       -0.01      ,  0.01      ,  0.01      ,  0.01      ,  0.01      ,\n",
              "        0.01      , -0.01      ,  0.01      ,  0.01      ,  0.01      ,\n",
              "       -0.01      ,  0.01      , -0.01      , -0.01      , -0.01      ,\n",
              "        0.01      ,  0.01      ,  0.01      , -0.01      , -0.01      ,\n",
              "       -0.01      ,  0.01      , -0.01      ,  0.01      , -0.01      ,\n",
              "        0.01      , -0.01      ,  0.01      , -0.01      ,  0.01      ,\n",
              "        0.01      ,  0.01      , -0.01      ,  0.01      , -0.01      ,\n",
              "       -0.01      ,  0.01      ,  0.01      , -0.01      ,  0.01      ,\n",
              "       -0.01      , -0.01      , -0.01      , -0.01      ,  0.01      ,\n",
              "        0.01      , -0.01      ,  0.01      ,  0.01      , -0.01      ,\n",
              "       -0.01      ,  0.01      ,  0.01      , -0.01      ,  0.01      ,\n",
              "        0.01      ,  0.01      , -0.01      ,  0.01      , -0.01      ,\n",
              "        0.01      , -0.01      , -0.01      ,  0.01      , -0.01      ,\n",
              "        0.01      , -0.01      , -0.01      ,  0.01      , -0.01      ,\n",
              "       -0.01      , -0.01      ,  0.01      , -0.01      ,  0.01      ,\n",
              "        0.01      , -0.01      , -0.01      ,  0.01      ,  0.01      ,\n",
              "        0.01      ,  0.01      ,  0.01      , -0.01      , -0.01      ,\n",
              "       -0.01      ,  0.01      ,  0.01      ,  0.01      ,  0.01      ,\n",
              "        0.01      , -0.01      ,  0.01      , -0.01      ,  0.01      ,\n",
              "        0.01      ,  0.01      ,  0.01      , -0.01      ], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47787a95"
      },
      "source": [
        "## Task\n",
        "Evaluate the trained Deep Neural Network (DNN) model using the preprocessed `x_test_reshaped` and `y_test_one_hot` data to obtain and present the accuracy, precision, and recall metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52d4d89f"
      },
      "source": [
        "### Evaluate Model\n",
        "\n",
        "#### Subtask:\n",
        "Evaluate the trained model using the preprocessed `x_test` and `y_test` data to obtain the accuracy, precision, and recall.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf3a03a3"
      },
      "source": [
        "**Reasoning**:\n",
        "To evaluate the trained model, I will use the `model.evaluate()` method with the preprocessed test data (`x_test_reshaped` and `y_test_one_hot`) and store the results in `evaluation_results`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "b616f749",
        "outputId": "82bf120d-1ecf-4077-a9de-a47297e0d692"
      },
      "source": [
        "evaluation_results = model.evaluate(x_test_reshaped, y_test_one_hot)\n",
        "\n",
        "print(\"Model Evaluation Results:\")\n",
        "print(f\"Loss: {evaluation_results[0]:.4f}\")\n",
        "print(f\"Accuracy: {evaluation_results[1]:.4f}\")\n",
        "print(f\"Precision: {evaluation_results[2]:.4f}\")\n",
        "print(f\"Recall: {evaluation_results[3]:.4f}\")"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "You must call `compile()` before using the model.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-753632194.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluation_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_reshaped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_one_hot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model Evaluation Results:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loss: {evaluation_results[0]:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Accuracy: {evaluation_results[1]:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/trainers/trainer.py\u001b[0m in \u001b[0;36m_assert_compile_called\u001b[0;34m(self, method_name)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"calling `{method_name}()`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_symbolic_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: You must call `compile()` before using the model."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a38f1fa"
      },
      "source": [
        "### Select and process a sample image\n",
        "\n",
        "#### Subtask:\n",
        "Select a sample image from the dataset, reshape it to 28x28 for plotting, and get its original predicted label.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4103bbb2",
        "outputId": "292b8bfa-e4c7-4675-e76b-c62bd76e1a43"
      },
      "source": [
        "sample_index = 0\n",
        "original_image = x_test_reshaped[sample_index]\n",
        "original_label_one_hot = y_test_one_hot[sample_index]\n",
        "\n",
        "# Reshape the image for plotting (28x28)\n",
        "original_image_reshaped = original_image.reshape(28, 28)\n",
        "\n",
        "# Predict the class of the original image\n",
        "original_image_batch = tf.expand_dims(original_image, axis=0)\n",
        "original_prediction = model.predict(original_image_batch)\n",
        "original_predicted_label = tf.argmax(original_prediction, axis=1).numpy()[0]\n",
        "\n",
        "print(f\"Original image index: {sample_index}\")\n",
        "print(f\"Original image shape (reshaped for plotting): {original_image_reshaped.shape}\")\n",
        "print(f\"Original one-hot encoded label: {original_label_one_hot}\")\n",
        "print(f\"Original predicted label: {original_predicted_label}\")"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "Original image index: 0\n",
            "Original image shape (reshaped for plotting): (28, 28)\n",
            "Original one-hot encoded label: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Original predicted label: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c614eac4"
      },
      "source": [
        "## Generate adversarial image\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "339a5593",
        "outputId": "83b5da41-a7a6-4be9-f9a6-af6128f12ac9"
      },
      "source": [
        "epsilon = 0.08\n",
        "adversarial_image = create_adversarial_image(original_image, original_label_one_hot, model, epsilon=epsilon)\n",
        "\n",
        "print(f\"Adversarial image generated with epsilon: {epsilon}\")"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[3.85707796e-13 5.11925169e-09 7.55146168e-09 6.55963461e-09\n",
            "  2.52213831e-13 9.99269352e-13 1.30210885e-14 1.00000000e+00\n",
            "  3.12003513e-11 2.62210254e-10]], shape=(1, 10), dtype=float32)\n",
            "Adversarial image generated with epsilon: 0.08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cdcb46c"
      },
      "source": [
        "## Reshape adversarial image for plotting\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5ebb577",
        "outputId": "73fec9dd-a3f2-4201-b10b-08703df47a59"
      },
      "source": [
        "adversarial_image_reshaped = adversarial_image.numpy().reshape(28, 28)\n",
        "# Clip the values to be between 0 and 1\n",
        "adversarial_image_reshaped = tf.clip_by_value(adversarial_image_reshaped, clip_value_min=0., clip_value_max=1.)\n",
        "\n",
        "print(f\"Shape of adversarial image reshaped for plotting: {adversarial_image_reshaped.shape}\")"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of adversarial image reshaped for plotting: (28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "192b84a2",
        "outputId": "4b2df6b0-c76f-4af3-a960-be198cb1a103"
      },
      "source": [
        "adversarial_image_batch = tf.expand_dims(adversarial_image, axis=0)\n",
        "adversarial_prediction = model.predict(adversarial_image_batch)\n",
        "adversarial_predicted_label = tf.argmax(adversarial_prediction, axis=1).numpy()[0]\n",
        "\n",
        "print(f\"Adversarial predicted label: {adversarial_predicted_label}\")"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "Adversarial predicted label: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "fd7e63da",
        "outputId": "ed0fcd83-5380-48c2-f203-53de154d3fe3"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Plot original image\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(original_image_reshaped, cmap='gray')\n",
        "plt.title(f\"Original Image\\nPredicted Label: {original_predicted_label}\")\n",
        "plt.axis('off')\n",
        "\n",
        "# Plot adversarial image\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(adversarial_image_reshaped, cmap='gray')\n",
        "plt.title(f\"Adversarial Image (epsilon={epsilon})\\nPredicted Label: {adversarial_predicted_label}\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"Original and adversarial images displayed with their predicted labels.\")"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original and adversarial images displayed with their predicted labels.\n"
          ]
        }
      ]
    }
  ]
}